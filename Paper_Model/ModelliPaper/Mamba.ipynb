{"cells":[{"cell_type":"code","source":["import torch\n","print(torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcIynbhndlnn","executionInfo":{"status":"ok","timestamp":1736764820903,"user_tz":-60,"elapsed":4591,"user":{"displayName":"Omar Coser","userId":"01334368924945290367"}},"outputId":"db8777e2-edf0-47f9-85d7-e9629ff48bd4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2.5.1+cu121\n"]}]},{"cell_type":"code","source":["pip install torchinfo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TUiYyoaq8muT","executionInfo":{"status":"ok","timestamp":1722233869508,"user_tz":-120,"elapsed":5049,"user":{"displayName":"Omar Coser","userId":"01334368924945290367"}},"outputId":"af7f5f86-d2af-4c56-f878-43adc7dcafa6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting torchinfo\n","  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n","Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.8.0\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# Assuming Mamba is defined in the mamba_ssm module\n","from mamba_ssm import Mamba\n","\n","# Define the wrapper class\n","class MambaWithClassification(nn.Module):\n","    def __init__(self, mamba_model, num_classes):\n","        super(MambaWithClassification, self).__init__()\n","        self.mamba_model = mamba_model\n","        self.classification_layer = nn.Linear(mamba_model.d_model, num_classes)\n","\n","    def forward(self, x):\n","        x = self.mamba_model(x)\n","        # Apply classification layer on the last dimension\n","        x = self.classification_layer(x)\n","        return x\n","\n","num_classes = 5  # Change this to the number of classes in your classification task\n","dim = 5\n","# Instantiate the Mamba model\n","mamba_model = Mamba(\n","    d_model=dim,  # Model dimension d_model\n","    d_state=16,   # SSM state expansion factor\n","    d_conv=4,     # Local convolution width\n","    expand=2      # Block expansion factor\n",").to(\"cuda\")\n","\n","# Wrap the Mamba model with the classification layer\n","model = MambaWithClassification(mamba_model, num_classes).to(\"cuda\")\n","\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","# Training loop\n","num_epochs = 10\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(\"cuda\"), labels.to(\"cuda\")\n","\n","        optimizer.zero_grad()\n","\n","        outputs = model(inputs)\n","        print(\"OT\")\n","        print(outputs)\n","        print(\"Fine OT\")\n","        # Assuming the output is of shape (batch_size, sequence_length, num_classes)\n","        outputs = outputs.mean(dim=1)  # Average over sequence length to match labels shape\n","        print(\"AfterMean\")\n","        print(outputs)\n","        print(\"Fine AfterMean\")\n","        print(\"label\")\n","        print(labels)\n","        print(\"fine labels\")\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss += loss.item()\n","\n","    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n","\n","print(\"Finished Training\")"],"metadata":{"id":"ssjHtqe38jZ1"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1B8NDtUApudZs7gwj_zGpiJW876zM58m3","timestamp":1700565923932}],"gpuType":"T4","authorship_tag":"ABX9TyMFpIdkScY7H5URw9CiPDW8"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}